# QuLab: Property-Based Testing for LLM Code Refinement

![QuLab Logo](https://www.quantuniversity.com/assets/img/logo5.jpg)

## Project Title

**QuLab: Iterative LLM Code Refinement with Property-Based Testing (PBT) vs. Test-Driven Development (TDD)**

## Description

This Streamlit application, "QuLab," serves as an interactive laboratory to explore and compare two powerful software testing methodologies—**Property-Based Testing (PBT)** and **Traditional Test-Driven Development (TDD)**—in the context of refining code generated by Large Language Models (LLMs).

LLMs are revolutionizing code generation, but their outputs can be prone to subtle errors, especially concerning edge cases or specific input distributions. Traditional unit tests often fall short in catching these nuances. This project demonstrates how PBT, by focusing on general properties rather than specific examples, can lead to significantly more robust and reliable LLM-generated code.

The application simulates an iterative refinement loop using a **PGS (Propose, Generate, Synthesize)** framework, where two LLM-powered agents (Generator and Tester) collaborate to improve code quality. Users can select HumanEval problems, choose a testing method (PBT or TDD), and observe the code's evolution and performance metrics across multiple refinement iterations.

## Features

*   **Interactive Application Overview**: Understand the fundamental concepts of PBT, TDD, and their relevance to LLM code refinement.
*   **HumanEval Dataset Integration**: Load and explore problems from the industry-standard HumanEval dataset for code generation benchmarking.
*   **Simulated LLM Agents**: Experience the iterative refinement process driven by mock Generator and Tester LLM agents.
*   **PBT vs. TDD Simulation**: Select between Property-Based Testing and Traditional Test-Driven Development to observe their distinct feedback mechanisms and impact on code evolution.
*   **Iterative Code Refinement Visualization**: Track the changes in LLM-generated code over multiple refinement iterations.
*   **Quantitative Comparison**: Evaluate the effectiveness of PBT and TDD using key metrics such as:
    *   `Correctness Rate`
    *   `Repair Success Rate`
    *   `Iterations to Solution`
    *   `Test Coverage Score` (PBT-specific)
    *   `Semantic Feedback Efficiency`
*   **Data-driven Visualizations**: Interactive charts and graphs to compare performance metrics and gain insights into the strengths of each testing paradigm.
*   **Summary and Insights**: Concluding remarks on the implications and future of PBT for AI-generated code.

## Getting Started

Follow these instructions to set up and run the QuLab application on your local machine.

### Prerequisites

*   Python 3.9+
*   `pip` (Python package installer)

### Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/your-username/qu-lab-pbt-llm.git
    cd qu-lab-pbt-llm
    ```
    *(Note: Replace `your-username/qu-lab-pbt-llm` with the actual repository path)*

2.  **Create a virtual environment (recommended)**:
    ```bash
    python -m venv venv
    source venv/bin/activate # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

    The `requirements.txt` file content would be:
    ```
    streamlit>=1.0.0
    pandas>=1.0.0
    datasets>=2.0.0
    plotly-express>=0.4.0
    ```

4.  **LLM API Keys (Optional)**:
    This lab uses mock LLM functions for demonstration purposes. In a real-world scenario, you would need to configure API keys for your chosen Large Language Model provider (e.g., OpenAI, Anthropic, Google Gemini). Instructions for configuring these would typically involve setting environment variables or using a configuration file, but are not strictly necessary to run this mock application.

## Usage

1.  **Run the Streamlit application**:
    ```bash
    streamlit run app.py
    ```
    This command will open the application in your default web browser (usually at `http://localhost:8501`).

2.  **Navigate the Application**:
    *   Use the sidebar on the left to navigate through different sections of the lab project.
    *   **1. Application Overview**: Introduction to the lab.
    *   **2. Environment Setup**: Review prerequisites and dependencies.
    *   **3. Data/Inputs Overview**: Load the HumanEval dataset and view its structure. **You must load the dataset here before proceeding to "Sectioned Implementation."**
    *   **4. Methodology Overview**: Deep dive into PBT, TDD, and the PGS framework.
    *   **5. Sectioned Implementation**: Select a HumanEval problem, view initial LLM code, choose between PBT or TDD, and run the iterative refinement loop.
    *   **6. Quantitative Comparison**: Run a mock benchmark evaluation across multiple problems for both PBT and TDD.
    *   **7. Visualization of Results**: Visualize the benchmark results with interactive charts.
    *   **8. Summary and Insights**: Read a summary of the findings and key takeaways.

## Project Structure

```
├── app.py                     # Main Streamlit application entry point
├── requirements.txt           # Python dependencies
└── application_pages/         # Directory for individual Streamlit page modules
    ├── page1_overview.py      # Introduces the application and learning goals
    ├── page2_env_setup.py     # Details environment setup (dependencies, etc.)
    ├── page3_data_inputs.py   # Handles HumanEval dataset loading and display
    ├── page4_methodology.py   # Explains PBT, TDD, and the PGS framework
    ├── page5_implementation.py# Interactive section for code refinement simulation
    ├── page6_quantitative_comparison.py # Runs mock benchmark evaluations
    ├── page7_visualization.py # Visualizes quantitative comparison results
    └── page8_summary.py       # Summarizes findings and provides insights
```

## Technology Stack

*   **Frontend/Framework**: [Streamlit](https://streamlit.io/)
*   **Language**: Python 3.9+
*   **Data Manipulation**: [Pandas](https://pandas.pydata.org/)
*   **Data Loading**: [Hugging Face `datasets`](https://huggingface.co/docs/datasets/)
*   **Visualization**: [Plotly Express](https://plotly.com/python/plotly-express/)

## Contributing

Contributions are welcome! If you'd like to improve this project, please follow these steps:

1.  Fork the repository.
2.  Create a new branch (`git checkout -b feature/your-feature-name`).
3.  Make your changes and ensure tests (if any) pass.
4.  Commit your changes (`git commit -m 'Add some feature'`).
5.  Push to the branch (`git push origin feature/your-feature-name`).
6.  Open a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
*(Note: A `LICENSE` file would need to be created in the root directory if this were a real project.)*

## Contact

For questions or feedback, please reach out to QuantUniversity:
*   **Website**: [https://www.quantuniversity.com](https://www.quantuniversity.com)
*   **Email**: info@quantuniversity.com

---
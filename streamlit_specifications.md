
# Streamlit Application Requirements Specification

## 1. Application Overview

This Streamlit application will provide an interactive demonstration and comparative analysis of Property-Based Testing (PBT) versus Traditional Test-Driven Development (TDD) for refining code generated by Large Language Models (LLMs). It will serve as a tool to visualize the iterative refinement process and evaluate the effectiveness of each testing paradigm.

### Learning Goals:
-   Understand the principles of Property-Based Testing (PBT) and how it differs from Traditional Test-Driven Development (TDD).
-   Learn how PBT improves the iterative refinement of code generated by Large Language Models (LLMs) compared to TDD.
-   Explore the HumanEval dataset as a benchmark for code generation tasks.
-   Implement the PGS framework involving two LLM-powered agents: Generator (code synthesis/refinement) and Tester (property validation/input generation).
-   Analyze and visualize how PBT-driven feedback leads to faster and more semantically accurate code refinement.
-   Demonstrate PBT effectiveness through comparative evaluations with TDD over multiple iterations.

## 2. User Interface Requirements

### Layout and Navigation Structure
The application will be structured using Streamlit's `st.sidebar` for main navigation and `st.tabs` or `st.expander` for sub-sections within a main content area.

-   **Sidebar Navigation:**
    -   "1. Application Overview"
    -   "2. Environment Setup"
    -   "3. Data/Inputs Overview"
    -   "4. Methodology Overview"
    -   "5. Sectioned Implementation" (main interactive area)
    -   "6. Quantitative Comparison"
    -   "7. Visualization of Results"
    -   "8. Summary and Insights"
-   **Main Content Area (Sectioned Implementation):** This section will dynamically load content and interactive elements based on user choices. Sub-sections will be presented using `st.expander` or `st.header` and `st.subheader`.

### Input Widgets and Controls
-   **Dataset Loading:**
    -   `st.button("Load HumanEval Dataset")`: Triggers the loading of the dataset.
-   **Problem Selection:**
    -   `st.selectbox("Select HumanEval Problem", options=humaneval_dataset_task_ids)`: Allows users to choose a specific problem from the loaded dataset.
    -   `st.number_input("Max Refinement Iterations", min_value=1, max_value=10, value=5)`: Controls the maximum iterations for refinement loops.
    -   `st.number_input("Test Timeout (seconds)", min_value=1, max_value=30, value=2)`: Sets the timeout for test execution.
-   **Testing Method Selection:**
    -   `st.radio("Select Testing Method", options=["Property-Based Testing (PBT)", "Traditional Test-Driven Development (TDD)"])`: Allows switching between PBT and TDD.
-   **Action Buttons:**
    -   `st.button("Run Refinement Loop")`: Initiates the chosen refinement process for the selected problem.
    -   `st.button("Run Benchmark Evaluation")`: Triggers the quantitative comparison on a subset of the dataset.

### Visualization Components (charts, graphs, tables)
-   **Raw Dataset Display:**
    -   `st.dataframe(humaneval_dataset['test'].to_pandas().head())`: Shows a sample of the HumanEval dataset.
-   **Code Display:**
    -   `st.code(code_string, language="python")`: Used to display initial LLM-generated code, property checks, and refined code attempts.
-   **Refinement History Table:**
    -   `st.dataframe(pd.DataFrame(refinement_history))`: Displays the iteration, code attempt (truncated), and detailed feedback.
-   **Quantitative Comparison Tables:**
    -   `st.dataframe(pbt_eval_df)` and `st.dataframe(tdd_eval_df)`: Show evaluation results for PBT and TDD on a benchmark subset.
    -   `st.dataframe(combined_eval_df)`: Displays combined results for easier comparison.
-   **Performance Comparison Plots (using `matplotlib.pyplot` and `seaborn`):**
    -   Line plots: `st.pyplot(fig_line)` for "Performance Trends Over Iterations" (e.g., `correctness_rate`, `repair_success_rate` vs. `Iteration`).
    -   Bar plots: `st.pyplot(fig_bar)` for "Average Performance Comparison" (average metrics for PBT vs. TDD).
    -   Bar plots for `visualize_test_coverage_and_feedback`: `st.pyplot(fig)` for "Test Pass Rate", "Property Violation Frequency", and "Semantic Feedback Efficiency".

### Interactive Elements and Feedback Mechanisms
-   **Loading/Processing Status:** `st.spinner("Loading dataset...")`, `st.info("Running refinement...")`, `st.success("Refinement successful!")`, `st.error("Refinement failed.")`.
-   **Detailed Feedback:** `st.expander("Detailed Feedback")` to reveal lists of `property_violations`, `runtime_errors`, and `timeouts`.
-   **Code History:** `st.expander("Code Attempts History")` to show previous versions of generated code.

## 3. Additional Requirements

### Annotation and Tooltip Specifications
-   **Mathematical Formulas:** All LaTeX equations will be rendered using `st.latex()` or `st.markdown` with appropriate delimiters. Tooltips/annotations for these will be provided in accompanying `st.markdown` text.
-   **Input Widgets:** `help` parameter will be used for `st.number_input`, `st.selectbox`, etc., to provide context (e.g., `st.number_input("Max Iterations", help="Maximum number of attempts the LLM will make to refine the code.")`).
-   **Visualizations:** Plot titles, axis labels, and legends will be clear. `st.pyplot` outputs will be accompanied by descriptive `st.markdown` text to interpret the graphs.

### Save the states of the fields properly so that changes are not lost
-   **Session State:** `st.session_state` will be extensively used to maintain the state of:
    -   `humaneval_dataset`: Once loaded.
    -   `selected_problem`: The currently chosen HumanEval problem.
    -   `max_iterations`: User-defined maximum iterations.
    -   `timeout_seconds`: User-defined test execution timeout.
    -   `current_code`: The LLM's latest code attempt.
    -   `refinement_history`: A list of dictionaries tracking code evolution and feedback.
    -   `pbt_eval_df`, `tdd_eval_df`, `combined_eval_df`: Results of benchmark evaluations.
    -   `selected_method`: The chosen testing method (PBT or TDD).
-   **Caching:**
    -   `@st.cache_data` will be used for `load_humaneval_dataset()` to prevent reloading on every rerun.
    -   `@st.cache_resource` might be considered if LLM models were actually loaded, but for this simulation, `cache_data` is sufficient for results.

## 4. Notebook Content and Code Requirements

All markdown content from the Jupyter notebook will be included in the Streamlit application using `st.markdown`. Mathematical formulas will be correctly formatted in LaTeX. Code stubs will be adapted to be callable functions within the Streamlit app.

### Markdown Content

```markdown
# Jupyter Notebook Specification: PBT vs TDD Code Refiner

---

## 1. Notebook Overview

This notebook serves as a comprehensive guide and comparative analysis between Property-Based Testing (PBT) and Traditional Test-Driven Development (TDD) in the context of refining code generated by Large Language Models (LLMs).

### Learning Goals:
- Understand the principles of Property-Based Testing (PBT) and how it differs from Traditional Test-Driven Development (TDD).
- Learn how PBT improves the iterative refinement of code generated by Large Language Models (LLMs) compared to TDD.
- Explore the HumanEval dataset as a benchmark for code generation tasks.
- Implement the PGS framework involving two LLM-powered agents: Generator (code synthesis/refinement) and Tester (property validation/input generation).
- Analyze and visualize how PBT-driven feedback leads to faster and more semantically accurate code refinement.
- Demonstrate PBT effectiveness through comparative evaluations with TDD over multiple iterations.

## 2. Environment Setup

This section prepares the environment by installing necessary libraries and importing modules. This ensures the notebook is self-contained and reproducible.

## 3. Data/Inputs Overview

Our evaluation framework relies on the **HumanEval dataset**, a widely recognized benchmark for evaluating the correctness of code generation models. This dataset is crucial for understanding how our LLM-driven refinement methods perform in practical coding scenarios.

### HumanEval Dataset Structure:
Each problem in the HumanEval dataset consists of the following key attributes:
- `task_id`: A unique identifier for the programming problem.
- `prompt`: The natural language problem description, which serves as the input for our LLM code generator.
- `canonical_solution`: The correct reference solution (used for evaluation, not directly by the LLM during generation).
- `test`: A string containing Python unit tests, often example-based, used for basic correctness checks.
- `entry_point`: The name of the function that the generated code is expected to implement.

By using HumanEval, we ensure that our evaluation is grounded in realistic code generation challenges, providing a solid foundation for comparing the effectiveness of PBT and TDD in refining LLM-generated code.

## 4. Methodology Overview

This notebook explores an iterative code refinement process driven by LLMs, comparing two distinct testing paradigms: Property-Based Testing (PBT) and Traditional Test-Driven Development (TDD). Our approach leverages a **Generator-Tester (PGS) framework** where a Generator LLM synthesizes and refines code, and a Tester LLM (or a heuristic agent in some parts) validates it.

### Property-Based Testing (PBT):
Property-Based Testing validates that generated code adheres to high-level semantic invariants (properties) for *all valid inputs* rather than specific input-output cases. This paradigm aims to discover a broader range of subtle bugs that example-based tests might miss.

### Traditional Test-Driven Development (TDD):
TDD focuses on writing specific, example-based test cases *before* writing the code. The code is then refined until it passes these predefined tests. While effective, TDD can sometimes suffer from a limited test oracle, where the provided examples may not cover all edge cases or semantic invariants.

### Key Formulas and Business Objectives:

1.  **Correctness Criterion (Traditional Tests):**
    This criterion ensures that the generated code produces the expected output for a given set of input-output pairs. From a business perspective, this translates directly to **functional correctness** and meeting basic user requirements.
    $$
    \forall (I_j, O_j) \in T_h, \quad C(I_j) = O_j
    $$
    Where:
    - $T_h$ represents the set of hidden test cases (e.g., public tests from HumanEval).
    - $I_j$ is an input, and $O_j$ is its corresponding expected output.
    - $C(I_j)$ is the output of the candidate code $C$ for input $I_j$.

2.  **PBT Property Validation:**
    This formula defines how PBT asserts that the code maintains fundamental truths (properties) across a diverse range of generated inputs. This is vital for **software robustness and reliability**, ensuring the code behaves correctly even in unforeseen scenarios, reducing post-deployment bugs and associated costs.
    $$
    \forall I \in \mathcal{D}, \quad P(C, I) = \text{True}
    $$
    Where:
    - $I$ represents an input generated from a diverse distribution $\mathcal{D}$.
    - $P$ is a property predicate (a test function) that captures invariants of the problem.
    - $C$ is the candidate code.

3.  **Iterative Refinement Stopping Condition:**
    This combined criterion dictates when the iterative refinement process (both PBT and TDD) can stop, indicating a successful solution. Achieving this state quickly means **faster development cycles** and **quicker time-to-market** for AI-generated code, while maintaining high quality.
    $$
    \text{Pass} := \bigwedge_{\forall i} C(I_i) = O_i \wedge \bigwedge_{\forall P_k} P_k(C, I_k) = \text{True}
    $$
    Where:
    - The first part checks correctness against traditional input-output pairs.
    - The second part verifies that all property-based checks ($P_k$) pass for their respective generated inputs ($I_k$).
    - The conjunction ($\bigwedge$) signifies that *all* conditions must be met for a successful solution.

## 5. Sectioned Implementation

This section details the implementation of each component of our PBT and TDD comparison framework. Each subsection provides context, relevant code, execution examples, and an interpretation of the results.

### Section 2: Loading the HumanEval Dataset

**Context & Business Value:**
To rigorously compare PBT and TDD for LLM-generated code, we need a standardized benchmark. The HumanEval dataset provides 164 carefully curated programming problems, each with a prompt, canonical solution, and a set of public tests. Loading this dataset is our first step, allowing us to feed consistent, real-world coding challenges to our LLM agents. This ensures our evaluations are fair and reflective of actual code generation scenarios, providing a reliable basis for improving the quality of AI-generated software.

**Interpretation:**
We have successfully loaded the HumanEval dataset and displayed a sample problem. The `prompt` clearly defines the coding task, the `test` field contains public unit tests, and `entry_point` specifies the function name to be implemented. This structured data is ready to be consumed by our Generator LLM, ensuring a consistent input for evaluating code generation and refinement.

### Section 3: Setup and Interaction with LLM Code Generator (Generator Agent)

**Context & Business Value:**
The Generator agent's role is to produce initial code solutions based on a given problem prompt. In a real-world application, this agent would interface with a sophisticated LLM (e.g., Codex, GPT-4). For this demonstration, we use a simulated LLM to generate placeholder code. This component is critical as it represents the initial automated attempt at solving a coding problem, directly impacting the starting quality and subsequent refinement effort. An efficient Generator leads to less iterative refinement, saving computational resources and accelerating development.

**Interpretation:**
The `generate_code_with_llm` function, though simulated, demonstrates how an LLM would take a problem prompt and produce a code snippet. In a production system, this step would be where the raw intelligence of the LLM translates natural language requirements into functional code. The quality of this initial generation directly influences the efficiency of subsequent refinement loops; a better initial guess means fewer iterations to reach a correct solution.

### Section 4: Property Definition and Generation (Tester Agent)

**Context & Business Value:**
The Tester agent's primary role in PBT is to define properties â€“ semantic invariants derived from the problem specification. These properties represent fundamental truths about the function's behavior that should hold true for all valid inputs. Unlike TDD's example-specific tests, PBT properties help avoid the flawed oracle problem by focusing on the underlying logic rather than specific input-output pairs. This leads to more robust and semantically accurate code, reducing the risk of subtle bugs in production.

**Formulae:**
Property formulation can be seen as defining a predicate $P$ that should always return `True` for any valid input $I$ when applied to the candidate code $C$.

$$ 
\forall I \in \mathcal{D}, \quad P(C, I) = \text{True}
$$

For example, a property for a sorting function might be:
$$ 
\forall \text{input } x, \text{ output } y = \text{sort}(x) \implies y_i \leq y_{i+1} \quad \text{for all } i
$$
This property ensures that the output is always sorted, regardless of the input array.

**Interpretation:**
We've generated three placeholder property-based test functions. In a fully realized system, an LLM would analyze the `problem_spec` and generate highly specific, semantic properties (e.g., for a sorting function: 'output is sorted', 'output has same elements as input', 'output length is same as input length'). These properties are invaluable because they validate the *meaning* of the code, not just its behavior on specific examples. This leads to discovering more profound logical errors that example-based tests often miss, enhancing the code's overall quality and reliability.

### Section 5: Property-Based Input Generation

**Context & Business Value:**
For Property-Based Testing to be effective, it requires a diverse and comprehensive set of inputs. These inputs should cover typical scenarios, edge cases (e.g., empty lists, zero, maximum values), and boundary conditions. Generating such a rich set of inputs is crucial for thoroughly exploring the behavioral space of the code and uncovering subtle bugs. This proactive approach to input generation directly contributes to **higher software quality and fewer production incidents**, as more potential failure modes are tested before deployment.

**Interpretation:**
The `generate_pbt_inputs` function demonstrates how diverse input sets can be generated based on the property specification. The examples show inputs tailored for sorting, integer, and general scenarios, including empty lists, single elements, negative numbers, and special characters. This diversity is crucial for PBT to effectively explore the input space and detect edge-case failures that fixed, example-based tests often overlook. By maximizing test coverage through varied inputs, we significantly improve the reliability of LLM-generated code.

### Section 6: Executing Tests and Property Checks on Candidate Code

**Context & Business Value:**
After code generation and property definition, the next critical step is to execute the candidate code against both traditional public tests and the newly generated property-based checks. This execution phase is where we gather concrete feedback on the code's correctness, robustness, and any potential flaws. The feedback is categorized into pass/fail status, property violations, wrong answers (for public tests), runtime errors, and timeouts. This detailed diagnostic information is invaluable for the Generator agent to understand *why* the code failed and how to refine it effectively, leading to faster debugging cycles and higher quality code with less manual intervention.

**Interpretation:**
The execution results clearly demonstrate the capabilities of the `run_tests` function:

- **Test Case 1 (All Pass):** Shows successful execution where the `add_one` function correctly passes its simple test.
- **Test Case 2 (Property Violation):** Highlights how a logical error (incorrect `is_even` implementation) leads to an `AssertionError`, specifically categorized as a `property_violation`. This feedback is crucial for guiding the Generator to fix semantic issues.
- **Test Case 3 (Runtime Error):** Demonstrates detection of a `ZeroDivisionError` as a `runtime_error`, providing detailed traceback information.
- **Test Case 4 (Timeout):** Successfully identifies and flags a long-running function as a `timeout`, preventing infinite loops from consuming excessive resources. This is essential for evaluating LLM-generated code which might sometimes produce inefficient solutions.
- **Test Case 5 (Syntax Error):** Catches a fundamental syntax error in the candidate code before any tests are even run, providing immediate feedback on malformed code.

This detailed feedback mechanism is central to the iterative refinement process, allowing LLM agents to receive precise signals on where and how their generated code is failing, thereby accelerating the path to correct and robust solutions.

### Section 7: Refinement Loop Using Property-Based Testing (PBT)

**Context & Business Value:**
The PBT refinement loop represents the core of our approach. In this iterative process, the Generator agent produces a candidate solution, which the Tester agent then rigorously evaluates using property-based checks and a diverse set of inputs. If any property violations or errors are detected, this rich semantic feedback is sent back to the Generator, prompting it to refine the code. This cycle continues until the code passes all properties and public tests, or a maximum iteration limit is reached. The business value here is significant: **faster convergence to semantically correct code**, leading to more reliable software, reduced manual debugging, and ultimately, a more efficient development pipeline for LLM-generated code.

**Formulae:**
The iterative refinement process stops when the code satisfies both traditional example-based tests and all property-based checks. This ensures both functional correctness and adherence to high-level semantic invariants.

$$
\text{Pass} := \bigwedge_{\forall i} C(I_i) = O_i \wedge \bigwedge_{\forall P_k} P_k(C, I_k) = \text{True}
$$

Where:
- $C(I_i) = O_i$ ensures the candidate code passes traditional example-based tests.
- $P_k(C, I_k) = \text{True}$ ensures the candidate code satisfies all property-based checks for diverse inputs.

**Interpretation:**
The PBT refinement loop demonstration shows the iterative nature of code generation, testing, and feedback-driven refinement. In our simulation, the process aims for convergence within a few iterations. Even with a simulated Generator, the output shows how feedback (e.g., property violations) would theoretically guide the Generator towards a correct solution. The history captures the evolution of the code and the feedback received at each step. This process highlights PBT's advantage: by focusing on semantic properties, it can provide richer, more actionable feedback than simple pass/fail tests, leading to a more efficient and targeted refinement trajectory. This ultimately saves time and resources in achieving high-quality, LLM-generated code.

### Section 8: Refinement Loop Using Traditional Test-Driven Development (TDD)

**Context & Business Value:**
In contrast to PBT, the Traditional Test-Driven Development (TDD) refinement loop relies solely on example-based test assertions. The Generator agent produces code, which is then tested against a fixed set of input-output examples (similar to the public tests in HumanEval). If these tests fail, the feedback is typically a pass/fail status or an assertion message, which the Generator uses to refine its code. This cycle continues until all example-based tests pass. While TDD is a proven methodology, its reliance on specific examples can limit its ability to detect broader semantic issues or edge cases that are not explicitly covered by the test set. Comparing this to PBT provides insights into the **completeness and efficiency of the feedback mechanism** when refining LLM-generated code, influencing the robustness and reliability of the final product.

**Interpretation:**
The TDD refinement loop demonstration mirrors the iterative process but with feedback primarily based on traditional example tests. The `iterate_tdd_refinement` function attempts to refine the code until all provided public tests pass. Similar to PBT, the `refine_code_with_feedback` function is called with feedback derived from test results. While the simulation will show a progression, in real scenarios, TDD might struggle to find subtle bugs if the example tests are not exhaustive. Comparing this to the PBT output, we can observe differences in the feedback provided (e.g., specific property violations vs. generic test failures) and potentially the number of iterations required for convergence, highlighting the distinct advantages of PBT's semantic validation.

### Section 9: Quantitative Comparison on Benchmark

**Context & Business Value:**
To move beyond anecdotal evidence, a quantitative evaluation on a benchmark like HumanEval is essential. This section focuses on running both PBT and TDD refinement methods across multiple HumanEval tasks and collecting key performance metrics. These metrics provide objective data to assess which method is more effective in refining LLM-generated code. Understanding `pass@1` and `Repair Success Rate (RSR)` directly translates to **efficiency in LLM development workflows** and **cost savings** by reducing the need for manual bug fixes and rework. By comparing these metrics, businesses can make informed decisions about which testing strategy yields the most reliable and efficient AI-powered code generation.

**Metrics:**
-   **pass@1:** The percentage of tasks for which the initial LLM generation (before any refinement) is correct on hidden tests. (While the evaluation is iterative, for the *method* comparison, we often look at the *final* pass rate after all iterations, or the rate at which it passes *within* iterations, which the `evaluate_method_on_benchmark` will capture as `correctness_rate`).
-   **Repair Success Rate (RSR):** The rate at which initially incorrect solutions are successfully corrected through the refinement loop (either PBT or TDD).

**Interpretation:**
The `evaluate_method_on_benchmark` function successfully ran our (simulated) PBT and TDD refinement loops over a mock HumanEval dataset. The resulting DataFrames provide quantitative metrics such as `iterations_to_pass`, `correctness_rate`, and `repair_success_rate` for each task and method. In this mock scenario, both methods show high success rates, but in a real-world setting with more complex LLM behavior and nuanced bugs, these metrics would reveal significant differences. This quantitative data is crucial for objectively comparing the two approaches and identifying which method provides superior performance in terms of efficiency and reliability for LLM code refinement.

### Section 10: Visualization of Results

**Context & Business Value:**
Visualizations are critical for translating raw quantitative data into actionable insights. By visually comparing PBT and TDD performance, we can easily identify trends, strengths, and weaknesses of each method. Plots showing correctness improvement over iterations, pass@1 rates, repair success rates, and failure mode analysis provide a clear narrative for stakeholders. These visualizations empower decision-makers to quickly grasp the impact of adopting PBT over TDD for LLM-generated code, leading to **better resource allocation, improved development strategies, and higher confidence in the quality of AI-assisted software development.**

**Interpretation:**
The generated visualizations clearly illustrate the comparative performance of PBT and TDD. The line plots demonstrate how metrics like `correctness_rate` and `repair_success_rate` evolve over refinement iterations for both methods. Typically, PBT is expected to show a steeper and higher improvement curve due to its semantically richer feedback. The bar charts for average performance consolidate these trends, offering a quick comparison. Furthermore, the test coverage and feedback visualizations highlight PBT's ability to uncover a higher frequency of violations and its superior feedback efficiency, indicating that PBT provides more actionable insights for LLM refinement. These visual insights are invaluable for demonstrating the tangible benefits of integrating PBT into LLM-driven code generation pipelines.

### Section 11: Summary and Insights

**Recap:**
This notebook has provided a comprehensive exploration of Property-Based Testing (PBT) versus Traditional Test-Driven Development (TDD) in the context of refining code generated by Large Language Models (LLMs). We've implemented a Generator-Tester framework, leveraging the HumanEval dataset to benchmark both approaches.

-   **Why PBT outperforms TDD in LLM-generated code refinement:** PBT's focus on high-level semantic invariants and diverse input generation allows it to uncover a broader spectrum of bugs, including subtle edge cases and logical flaws that example-based TDD tests might miss. The richer, property-driven feedback provided by PBT guides LLMs to more robust and semantically accurate solutions faster.
-   **The importance of property-driven validation for semantic correctness:** By validating fundamental truths (properties) about a function's behavior, PBT ensures that LLM-generated code not only passes specific examples but also adheres to its intended meaning across a vast input space. This significantly enhances the reliability and trustworthiness of AI-generated software.

**Practical considerations for applying PBT in LLM coding workflows:**
-   **Initial Property Definition:** Crafting effective properties is crucial. This can be aided by LLMs themselves, generating properties from problem descriptions.
-   **Input Generation Diversity:** Ensuring truly diverse and challenging inputs is key. Hybrid approaches combining LLM-generated inputs with heuristic or symbolic execution can maximize coverage.
-   **Feedback Interpretation:** The LLM Generator needs to effectively interpret property violation feedback to make targeted refinements.

**Future directions and research questions enabled by PGS framework:**
-   **Adaptive Property Generation:** Can LLMs learn to generate more effective properties over time, based on observed code failures?
-   **Hybrid Testing Strategies:** Exploring optimal combinations of PBT, TDD, and formal verification methods for different types of code generation tasks.
-   **Scalability and Efficiency:** Optimizing the performance of iterative refinement loops, especially when dealing with large codebases or complex properties.
-   **Human-in-the-Loop Integration:** How can human developers most effectively guide and oversee LLM-driven PBT refinement to ensure safety and correctness in critical applications?

By systematically comparing PBT and TDD, this work contributes to building more reliable and robust AI-powered code generation systems, ultimately accelerating software development and improving code quality.
```

### Extracted Code Stubs

The following Python functions and classes will be extracted and used in the Streamlit application. Necessary imports will be grouped at the top of the Streamlit script.

**Initial Setup (to be run once in the environment or Streamlit's global scope):**
```python
# Install required libraries (outside the Streamlit app, or handled by a requirements.txt)
# !pip install datasets pandas matplotlib seaborn
# !pip install hypothesis # If dynamic property-based input generation is to be fully integrated

import datasets
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import collections.abc
import multiprocessing
import queue
import ast
import traceback
import sys
from typing import List, Any
import json
import time
import random
import logging
import warnings
import streamlit as st

# Configure logging (optional, but good practice for Streamlit debugging)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
warnings.filterwarnings('ignore')
sns.set_theme(style="whitegrid")
```

**Section 2: Loading the HumanEval Dataset**
```python
@st.cache_data
def load_humaneval_dataset():
    """Loads the HumanEval dataset.

    Returns:
        Dataset: The HumanEval dataset.
    """
    logging.info("Loading HumanEval dataset...")
    dataset = datasets.load_dataset("openai_humaneval")
    logging.info("HumanEval dataset loaded successfully.")
    return dataset
```

**Section 3: Setup and Interaction with LLM Code Generator (Generator Agent)**
```python
def generate_code_with_llm(prompt):
    """Interacts with a Large Language Model (LLM) to generate code based on a problem prompt.
    (Note: This implementation simulates LLM behavior for testing purposes.)
    """
    if not isinstance(prompt, str):
        raise TypeError("Prompt must be a string.")

    if prompt == "":
        return ""
    else:
        logging.info(f"Simulating LLM code generation for prompt: '{prompt[:50]}...' ")
        return f"# Code generated by LLM for prompt: '{prompt}'\n\ndef generated_function():\n    # Placeholder for LLM-generated logic\n    pass\n"
```

**Section 4: Property Definition and Generation (Tester Agent)**
```python
def generate_property_checks(problem_spec):
    """Generates property-based test functions (as strings) representing semantic invariants
    derived from the problem specification using an LLM.

    Arguments:
        problem_spec (str): The problem specification.
    Output:
        List[str]: A list of strings, where each string is a Python function representing a property check.
    """
    if not isinstance(problem_spec, str):
        raise TypeError("problem_spec must be a string.")

    if not problem_spec:
        return []

    logging.info(f"Simulating LLM property check generation for spec: '{problem_spec[:50]}...' ")
    return [
        "def property_check_output_type(candidate_func, data):\n    # Placeholder: Checks if the output has the expected type.\n    # For demonstration, let's assume `data` is a tuple (input, expected_type)\n    # and the function `candidate_func` takes a single argument.\n    # A real property check would be more sophisticated.\n    if isinstance(data, tuple) and len(data) == 2:\n        input_val, expected_type = data\n        try:\n            result = candidate_func(input_val)\n            assert isinstance(result, expected_type), f\"Output type mismatch: Expected {expected_type}, got {type(result)}\"\n        except Exception as e:\n            raise AssertionError(f\"Candidate function failed during type check: {e}\") from e\n    else:\n        # For simpler single-value inputs, just assume it should be an int for this dummy.\n        try:\n            result = candidate_func(data)\n            assert isinstance(result, int), f\"Output type mismatch: Expected int, got {type(result)}\"\n        except Exception as e:\n            raise AssertionError(f\"Candidate function failed during type check: {e}\") from e\n",
        "def property_check_invariance(candidate_func, data):\n    # Placeholder: Checks for preservation of certain properties (e.g., length, elements).\n    # For demonstration, assume `data` is an input list, and a 'sort' function is being tested.\n    if isinstance(data, list):\n        original_length = len(data)\n        original_elements = sorted(data) # To compare elements later, ignoring order\n        try:\n            result = candidate_func(data)\n            assert isinstance(result, list), \"Output is not a list.\"\n            assert len(result) == original_length, \"Length changed after operation.\"\n            assert sorted(result) == original_elements, \"Elements changed or lost after operation.\"\n        except Exception as e:\n            raise AssertionError(f\"Candidate function failed during invariance check: {e}\") from e\n    else:\n        # For non-list inputs, let's just make a dummy passing check.\n        pass\n",
        "def property_check_postcondition(candidate_func, data):\n    # Placeholder: Checks if postconditions are met after operation.\n    # For demonstration, let's assume `data` is an input for a function that should always return positive for positive input.\n    if isinstance(data, int) and data > 0:\n        try:\n            result = candidate_func(data)\n            assert result > 0, f\"Postcondition failed: Expected positive result for positive input, got {result}\"\n        except Exception as e:\n            raise AssertionError(f\"Candidate function failed during postcondition check: {e}\") from e\n    else:\n        # For other inputs, assume it passes for this dummy.\n        pass\n"
    ]
```

**Section 5: Property-Based Input Generation**
```python
def generate_pbt_inputs(property_spec: str) -> List[Any]:
    """Generates diverse input test cases for validating the properties, covering typical, edge, and boundary conditions.

    Arguments:
        property_spec (str): The specification of the property to generate inputs for.

    Returns:
        List[Any]: A list of input values suitable for testing the property.
    """
    if not isinstance(property_spec, str):
        raise TypeError("property_spec must be a string.")

    if property_spec == "":
        return []

    logging.info(f"Generating PBT inputs for property spec: '{property_spec[:50]}...' ")
    lower_spec = property_spec.lower()

    if "sorting" in lower_spec or "sort" in lower_spec or "array" in lower_spec:
        return [[], [1], [1, 2, 3], [3, 2, 1], [-1, 0, 1], [5, 2, 8, 1, 9, 2], [100, -50, 0, 75, -25], [7, 7, 7]]
    elif "binary tree" in lower_spec or "tree" in lower_spec:
        return [
            None,  # Empty tree
            [1],  # Single-node tree
            [1, None, 2],  # Unbalanced right
            [2, 1, None],  # Unbalanced left
            [3, 1, 5, None, None, 4, 6],  # More complex, potentially unbalanced
            [10, 5, 15, 2, 7, 12, 17] # Balanced example
        ]
    elif "integer" in lower_spec or "number" in lower_spec:
        return [0, 1, -1, 100, -100, 2**31 - 1, -(2**31), 2**63 - 1, -(2**63), 0.0, -0.0]
    elif "string" in lower_spec or "text" in lower_spec:
        return ["", "a", "hello", "World", "longer string with spaces", "special!@#$%^&*()_+-=", "æ—¥æœ¬èªž", "ðŸ˜Ž"]
    elif "boolean" in lower_spec or "bool" in lower_spec:
        return [True, False]
    else:
        return ["typical_input_A", "typical_input_B", "edge_case_X", 0, 1, -1, True, False, [], None, {}, ""]
```

**Section 6: Executing Tests and Property Checks on Candidate Code**
```python
# TIMEOUT_SECONDS will be a user-configurable st.number_input
# TIMEOUT_SECONDS = 2 

def _worker_test_runner(code_str, test_str, input_val, result_queue, entry_point):
    """
    Executes a single test case within a separate process.
    It re-executes the candidate code and the test code strings to ensure
    isolation of environments and to make them available in the worker's scope.
    Captures success, property violations (AssertionError), or other runtime errors.
    """
    worker_env = {}
    
    try:
        # Re-execute candidate code string to define the function in the worker's environment.
        exec(code_str, worker_env)
        
        candidate_func = worker_env.get(entry_point)
        if not candidate_func:
            raise ValueError(f"Candidate function '{entry_point}' not found in worker environment after execution of candidate code.")

        # Re-execute test code string to define the test function in the worker's environment.
        # The HumanEval test string typically defines a `check` function.
        exec(test_str, worker_env)
        
        test_func = worker_env.get("check") # HumanEval public tests typically use 'check'
        if not test_func:
            # If it's not a 'check' function (e.g., for generated properties), try to infer
            test_tree = ast.parse(test_str)
            for node in test_tree.body:
                if isinstance(node, ast.FunctionDef):
                    test_func_name = node.name
                    test_func = worker_env.get(test_func_name)
                    break
            if not test_func:
                 raise ValueError(f"Test function not found in test string.")

        # Execute the test function with the candidate function and input.
        # HumanEval 'check' functions take `candidate` as the first arg.
        # Property checks often take `candidate_func, data`.
        if "candidate_func" in test_str and "data" in test_str: # Simple heuristic for property checks
            test_func(candidate_func, input_val)
        else: # Assume traditional check function signature
            test_func(candidate_func) 
        
        result_queue.put(("success", None))

    except AssertionError as e:
        result_queue.put(("property_violation", str(e)))
    except Exception as e:
        result_queue.put(("runtime_error", traceback.format_exc()))


def run_tests(code, tests, inputs, entry_point, timeout_seconds):
    """
    Executes the candidate code with generated tests and inputs, returning detailed feedback
    including pass/fail status, property violations, runtime errors, and timeouts.

    Arguments:
        code (str): The code to be tested, expected to contain a single function definition.
        tests (List[str]): A list of test functions (as strings), each defining a test.
        inputs (List[Any]): A list of input values for the tests.
        entry_point (str): The name of the main function in the candidate code.
        timeout_seconds (int): Timeout for each test execution.

    Output:
        Dict: A dictionary containing detailed feedback on the test execution.
    """
    if not isinstance(code, str): raise TypeError("Code must be a string.")
    if not isinstance(tests, list) or not all(isinstance(t, str) for t in tests): raise TypeError("Tests must be a list of strings.")
    if not isinstance(inputs, list): raise TypeError("Inputs must be a list.")
    if not isinstance(entry_point, str): raise TypeError("Entry point must be a string.")
    if not isinstance(timeout_seconds, (int, float)) or timeout_seconds <= 0: raise ValueError("Timeout seconds must be a positive number.")

    feedback = {
        "passed": True,
        "property_violations": [],
        "runtime_errors": [],
        "timeouts": []
    }

    try:
        ast.parse(code)
    except SyntaxError as e:
        feedback["passed"] = False
        feedback["runtime_errors"].append(f"Syntax error in candidate code: {e}")
        return feedback
    
    if not tests or not inputs:
        return feedback

    for test_index, test_str in enumerate(tests):
        test_func_name = f"anonymous_test_{test_index}"
        try:
            test_tree = ast.parse(test_str)
            for node in test_tree.body:
                if isinstance(node, ast.FunctionDef):
                    test_func_name = node.name
                    break
        except SyntaxError as e:
            feedback["passed"] = False
            feedback["runtime_errors"].append(f"Syntax error in test code string '{test_func_name}': {e}")
            continue

        for input_index, input_val in enumerate(inputs):
            q = multiprocessing.Queue()
            process = multiprocessing.Process(
                target=_worker_test_runner,
                args=(code, test_str, input_val, q, entry_point)
            )
            process.start()
            process.join(timeout=timeout_seconds)

            if process.is_alive():
                process.terminate()
                process.join()
                feedback["timeouts"].append(
                    f"Test '{test_func_name}' with input {input_val!r} (input index {input_index}) timed out after {timeout_seconds}s."
                )
                feedback["passed"] = False
            else:
                try:
                    outcome_type, message = q.get(timeout=0.5) 
                    if outcome_type == "property_violation":
                        feedback["property_violations"].append(
                            f"Test '{test_func_name}' with input {input_val!r} (input index {input_index}) failed assertion: {message}"
                        )
                        feedback["passed"] = False
                    elif outcome_type == "runtime_error":
                        feedback["runtime_errors"].append(
                            f"Test '{test_func_name}' with input {input_val!r} (input index {input_index}) raised runtime error: {message}"
                        )
                        feedback["passed"] = False
                except queue.Empty:
                    feedback["runtime_errors"].append(
                        f"Test '{test_func_name}' with input {input_val!r} (input index {input_index}) failed to report result from worker process. Possible unexpected termination or IPC issue."
                    )
                    feedback["passed"] = False
                except Exception as e:
                    feedback["runtime_errors"].append(
                        f"Error processing result from worker for test '{test_func_name}' with input {input_val!r}: {e}"
                    )
                    feedback["passed"] = False
            
            while not q.empty():
                try: q.get_nowait()
                except queue.Empty: break

    return feedback
```

**Section 7: Refinement Loop Using Property-Based Testing (PBT)**
```python
def refine_code_with_feedback(buggy_code, feedback):
    """
    Refines code based on feedback. Simulates LLM behavior for testing purposes.
    """
    if not isinstance(buggy_code, str): raise TypeError("buggy_code must be a string.")
    if not isinstance(feedback, str): raise TypeError("feedback must be a string.")

    if not buggy_code and "factorial" in feedback.lower():
        logging.info("Simulating LLM refining for factorial function.")
        return (
            "def factorial(n):\n"
            "    \"\"\"Calculates the factorial of a non-negative integer.\"\"\"\n"
            "    if not isinstance(n, int) or n < 0:\n"
            "        raise ValueError(\"Input must be a non-negative integer.\")\n"
            "    if n == 0:\n"
            "        return 1\n"
            "    else:\n"
            "        return n * factorial(n - 1)\n"
        )
    
    logging.info(f"Simulating LLM refinement: returning original code with feedback: {feedback.replace('\n', ' ')[:100]}...")
    # For a more illustrative simulation, we can append feedback as a comment.
    return buggy_code + f"\n# Refined based on feedback: {feedback.replace('\n', ' ')[:100]}...\n"


class MockDatasetEntry:
    """A mock class to simulate a DatasetEntry object for demonstration purposes."""
    def __init__(self, task_id, prompt, test_code_str, entry_point, canonical_solution=""):
        self.task_id = task_id
        self.prompt = prompt
        self.test = test_code_str # Original test string from HumanEval
        self.entry_point = entry_point
        self.canonical_solution = canonical_solution
    
    @property
    def tests(self): 
        return [self.test]
    
    @property
    def inputs(self):
        if "add two numbers" in self.prompt.lower():
            return [[1, 2], [0, 0], [-1, 5]]
        elif "sort" in self.prompt.lower():
            return [[3, 1, 2], [5, 5], []]
        else:
            return [1, 5, 0, -1, True] # Generic inputs for demo


def iterate_pbt_refinement(problem, max_iterations, timeout_seconds):
    """Runs the Generator-Tester iterative loop using Property-Based Testing (PBT)."""
    required_attributes = ['prompt', 'test', 'entry_point']
    if not all(hasattr(problem, attr) for attr in required_attributes):
        if problem is None or isinstance(problem, (int, str)):
            raise TypeError(
                f"Problem input must be an object with '{', '.join(required_attributes)}' "
                f"attributes, but received {type(problem).__name__}."
            )
        else:
            missing_attrs = [attr for attr in required_attributes if not hasattr(problem, attr)]
            raise AttributeError(
                f"Problem object of type {type(problem).__name__} lacks required "
                f"attributes: {', '.join(missing_attrs)}."
            )

    logging.info(f"Starting PBT refinement for task: {problem.task_id}")

    current_code = generate_code_with_llm(problem.prompt) # Generator's initial code
    refinement_history = []

    for i in range(max_iterations):
        logging.info(f"PBT Iteration {i + 1}/{max_iterations}")

        property_specs = generate_property_checks(problem.prompt)
        pbt_inputs = generate_pbt_inputs(problem.prompt) 

        dummy_pbt_test = """def property_dummy_check(candidate_func, data):
    # Simulate a property check: e.g., if function always returns a non-negative number for non-negative inputs
    if isinstance(data, int) and data >= 0:
        result = candidate_func(data)
        assert result >= 0, f\"Property violation: Expected non-negative result for {data}, got {result}\"
    else:
        pass
"""
        combined_tests = [problem.test, dummy_pbt_test] 
        
        feedback = run_tests(current_code, combined_tests, pbt_inputs, problem.entry_point, timeout_seconds)
        
        refinement_history.append({
            "iteration": i + 1,
            "code_attempt": current_code,
            "feedback": feedback
        })

        if feedback["passed"]:
            logging.info(f"PBT refinement successful in {i + 1} iterations.")
            return {"success": True, "iterations": i + 1, "final_code": current_code, "history": refinement_history}

        feedback_str = f"Property violations: {feedback['property_violations']}\nRuntime errors: {feedback['runtime_errors']}\nTimeouts: {feedback['timeouts']}"
        current_code = refine_code_with_feedback(current_code, feedback_str)

    logging.warning(f"PBT refinement failed to converge within {max_iterations} iterations for task: {problem.task_id}")
    return {"success": False, "iterations": max_iterations, "final_code": current_code, "history": refinement_history}
```

**Section 8: Refinement Loop Using Traditional Test-Driven Development (TDD)**
```python
def iterate_tdd_refinement(problem, max_iterations, timeout_seconds):
    """Runs a similar iterative code refinement loop as `iterate_pbt_refinement`,
    but using traditional example-based test assertions instead of property-based tests for feedback (Test-Driven Development).
    """
    required_attributes = ['prompt', 'test', 'entry_point']
    if not all(hasattr(problem, attr) for attr in required_attributes):
        if problem is None or isinstance(problem, (int, str)):
            raise TypeError(
                f"Problem input must be an object with '{', '.join(required_attributes)}' "
                f"attributes, but received {type(problem).__name__}."
            )
        else:
            missing_attrs = [attr for attr in required_attributes if not hasattr(problem, attr)]
            raise AttributeError(
                f"Problem object of type {type(problem).__name__} lacks required "
                f"attributes: {', '.join(missing_attrs)}."
            )

    logging.info(f"Starting TDD refinement for task: {problem.task_id}")

    current_code = generate_code_with_llm(problem.prompt)
    refinement_history = []

    for iteration in range(max_iterations):
        logging.info(f"TDD Iteration {iteration + 1}/{max_iterations}")

        tdd_tests = [problem.test] 
        tdd_inputs = problem.inputs 
        
        test_results = run_tests(current_code, tdd_tests, tdd_inputs, problem.entry_point, timeout_seconds)

        refinement_history.append({
            "iteration": iteration + 1,
            "code_attempt": current_code,
            "feedback": test_results 
        })

        if test_results["passed"]:
            logging.info(f"TDD refinement successful in {iteration + 1} iterations.")
            return {"success": True, "iterations": iteration + 1, "final_code": current_code, "history": refinement_history}

        feedback_str = f"Test failures: {test_results['runtime_errors']}\nTimeouts: {test_results['timeouts']}\nProperty violations (if any detected by run_tests, even if not PBT-driven): {test_results['property_violations']}"
        current_code = refine_code_with_feedback(current_code, feedback_str)

    logging.warning(f"TDD refinement failed to converge within {max_iterations} iterations for task: {problem.task_id}")
    return {"success": False, "iterations": max_iterations, "final_code": current_code, "history": refinement_history}
```

**Section 9: Quantitative Comparison on Benchmark**
```python
def evaluate_method_on_benchmark(method, dataset, num_iterations, timeout_seconds):
    """
    Runs refinement methods (PBT/TDD) on HumanEval tasks, recording metrics.
    """
    if not callable(method): raise TypeError("Argument 'method' must be callable.")
    if not isinstance(num_iterations, int): raise TypeError("Argument 'num_iterations' must be an integer.")
    if num_iterations < 0: raise ValueError("Argument 'num_iterations' must be non-negative.")
    if not (isinstance(dataset, collections.abc.Sized) and isinstance(dataset, collections.abc.Iterable)): raise TypeError("Argument 'dataset' must be an iterable and sized collection.")

    results_data = []
    columns = [
        'task_id', 'method_name', 'iterations_to_pass',
        'correctness_rate', 'repair_success_rate'
    ]

    logging.info(f"Evaluating method '{getattr(method, '__name__', str(method))}' on benchmark with {len(dataset)} tasks.")

    for i, task in enumerate(dataset):
        task_id = getattr(task, 'task_id', f'task_{i+1}')
        method_name = getattr(method, '__name__', str(method))
        
        try:
            refinement_output = method(task, num_iterations, timeout_seconds) # Pass max_iterations and timeout
            
            if refinement_output and refinement_output['success']:
                iterations = refinement_output['iterations']
                correctness_rate = 1.0 
                repair_success = 1.0   
            else:
                iterations = num_iterations 
                correctness_rate = 0.0
                repair_success = 0.0
                
        except Exception as e:
            logging.error(f"Error running {method_name} on task {task_id}: {e}")
            iterations = num_iterations 
            correctness_rate = 0.0
            repair_success = 0.0

        metrics = {
            'task_id': task_id,
            'method_name': method_name,
            'iterations_to_pass': iterations,
            'correctness_rate': correctness_rate,
            'repair_success_rate': repair_success,
        }
        results_data.append(metrics)

    return pd.DataFrame(results_data, columns=columns)
```

**Section 10: Visualization of Results**
```python
def visualize_performance_comparison(results_df):
    """
    Generates plots comparing PBT vs TDD on metrics like pass rate, repair success,
    number of iterations, property violations over time.
    """
    if not isinstance(results_df, pd.DataFrame): raise TypeError("results_df must be a pandas DataFrame.")
    if results_df.empty:
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.text(0.5, 0.5, "No data to visualize.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='gray')
        ax.set_title("Performance Comparison (Empty Data)")
        ax.axis('off') 
        return fig

    required_columns = ['Iteration', 'Method', 'Metric', 'Value']
    for col in required_columns:
        if col not in results_df.columns:
            raise KeyError(f"Missing required column: '{col}' in results_df. Cannot generate plots.")

    unique_metrics = results_df['Metric'].unique()
    num_metrics = len(unique_metrics)
    cols = 2 
    rows = (num_metrics + cols - 1) // cols 

    # --- Plotting Trends Over Iterations (Line Plots) ---
    fig_line, axes_line = plt.subplots(rows, cols, figsize=(12, 5 * rows), squeeze=False)
    fig_line.suptitle("Performance Trends Over Iterations", fontsize=16, y=1.02)
    axes_line_flat = axes_line.flatten()

    for i, metric in enumerate(unique_metrics):
        ax = axes_line_flat[i]
        metric_df = results_df[results_df['Metric'] == metric]
        sns.lineplot(data=metric_df, x='Iteration', y='Value', hue='Method', marker='o', errorbar='sd', ax=ax)
        ax.set_title(f'{metric} Over Time', fontsize=12)
        ax.set_xlabel('Iteration', fontsize=10)
        ax.set_ylabel(metric, fontsize=10)
        ax.grid(True, linestyle='--', alpha=0.7)
        ax.legend(title='Method', loc='best')
    
    for j in range(i + 1, len(axes_line_flat)):
        fig_line.delaxes(axes_line_flat[j])

    fig_line.tight_layout(rect=[0, 0.03, 1, 0.98]) 

    # --- Plotting Overall Performance Comparison (Bar Plots) ---
    avg_performance = results_df.groupby(['Metric', 'Method'], as_index=False)['Value'].mean()

    fig_bar, axes_bar = plt.subplots(rows, cols, figsize=(12, 5 * rows), squeeze=False)
    fig_bar.suptitle("Average Performance Comparison", fontsize=16, y=1.02)
    axes_bar_flat = axes_bar.flatten()

    for i, metric in enumerate(unique_metrics):
        ax = axes_bar_flat[i]
        metric_avg_df = avg_performance[avg_performance['Metric'] == metric]
        sns.barplot(data=metric_avg_df, x='Method', y='Value', hue='Method', palette='viridis', ax=ax, errorbar='sd')
        ax.set_title(f'Average {metric}', fontsize=12)
        ax.set_xlabel('Method', fontsize=10)
        ax.set_ylabel(f'Average {metric}', fontsize=10)
        ax.grid(axis='y', linestyle='--', alpha=0.7)
        ax.legend(title='Method', loc='best')

    for j in range(i + 1, len(axes_bar_flat)): 
        fig_bar.delaxes(axes_bar_flat[j]) 

    fig_bar.tight_layout(rect=[0, 0.03, 1, 0.98]) 

    return fig_line, fig_bar


def visualize_test_coverage_and_feedback(results_df):
    """
    Shows charts/tables of test coverage, property violation frequency, semantic feedback efficiency.
    """
    if not isinstance(results_df, pd.DataFrame): raise TypeError("Input 'results_df' must be a pandas DataFrame.")
    
    if results_df.empty:
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.text(0.5, 0.5, "No data to visualize.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='gray')
        ax.set_title("Test Coverage (Empty Data)")
        ax.axis('off')
        return fig

    required_columns = ['method', 'pass_rate', 'violation_frequency', 'feedback_efficiency']
    missing_columns = [col for col in required_columns if col not in results_df.columns]
    if missing_columns:
        raise ValueError(f"DataFrame is missing required columns: {', '.join(missing_columns)}")

    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15))
    fig.suptitle('Test Coverage, Property Violation & Semantic Feedback Analysis', fontsize=16)

    sns.barplot(x='method', y='pass_rate', data=results_df, ax=axes[0])
    axes[0].set_title('Test Pass Rate by Method')
    axes[0].set_ylabel('Pass Rate')
    axes[0].set_xlabel('Method')
    axes[0].set_ylim(0, 1) 

    max_violation_freq = results_df['violation_frequency'].max()
    y_limit_violation = max(max_violation_freq * 1.1, 0.1)

    sns.barplot(x='method', y='violation_frequency', data=results_df, ax=axes[1])
    axes[1].set_title('Property Violation Frequency by Method')
    axes[1].set_ylabel('Violation Frequency')
    axes[1].set_xlabel('Method')
    axes[1].set_ylim(0, y_limit_violation)

    sns.barplot(x='method', y='feedback_efficiency', data=results_df, ax=axes[2])
    axes[2].set_title('Semantic Feedback Efficiency by Method')
    axes[2].set_ylabel('Feedback Efficiency')
    axes[2].set_xlabel('Method')
    axes[2].set_ylim(0, 1)

    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    return fig
```


# Streamlit Application Requirements Specification

## 1. Application Overview

This Streamlit application will provide an interactive demonstration and comparative analysis of Property-Based Testing (PBT) versus Traditional Test-Driven Development (TDD) for refining code generated by Large Language Models (LLMs). It will serve as a tool to visualize the iterative refinement process and evaluate the effectiveness of each testing paradigm.

### Learning Goals:
-   Understand the principles of Property-Based Testing (PBT) and how it differs from Traditional Test-Driven Development (TDD).
-   Learn how PBT improves the iterative refinement of code generated by Large Language Models (LLMs) compared to TDD.
-   Explore the HumanEval dataset as a benchmark for code generation tasks.
-   Implement the PGS framework involving two LLM-powered agents: Generator (code synthesis/refinement) and Tester (property validation/input generation).
-   Analyze and visualize how PBT-driven feedback leads to faster and more semantically accurate code refinement.
-   Demonstrate PBT effectiveness through comparative evaluations with TDD over multiple iterations.

## 2. User Interface Requirements

### Layout and Navigation Structure
The application will be structured using Streamlit's `st.sidebar` for main navigation and `st.tabs` or `st.expander` for sub-sections within a main content area.

-   **Sidebar Navigation:**
    -   "1. Application Overview"
    -   "2. Environment Setup"
    -   "3. Data/Inputs Overview"
    -   "4. Methodology Overview"
    -   "5. Sectioned Implementation" (main interactive area)
    -   "6. Quantitative Comparison"
    -   "7. Visualization of Results"
    -   "8. Summary and Insights"
-   **Main Content Area (Sectioned Implementation):** This section will dynamically load content and interactive elements based on user choices. Sub-sections will be presented using `st.expander` or `st.header` and `st.subheader`.

### Input Widgets and Controls
-   **Dataset Loading:**
    -   `st.button("Load HumanEval Dataset")`: Triggers the loading of the dataset.
-   **Problem Selection:**
    -   `st.selectbox("Select HumanEval Problem", options=humaneval_dataset_task_ids)`: Allows users to choose a specific problem from the loaded dataset.
    -   `st.number_input("Max Refinement Iterations", min_value=1, max_value=10, value=5)`: Controls the maximum iterations for refinement loops.
    -   `st.number_input("Test Timeout (seconds)", min_value=1, max_value=30, value=2)`: Sets the timeout for test execution.
-   **Testing Method Selection:**
    -   `st.radio("Select Testing Method", options=["Property-Based Testing (PBT)", "Traditional Test-Driven Development (TDD)"])`: Allows switching between PBT and TDD.
-   **Action Buttons:**
    -   `st.button("Run Refinement Loop")`: Initiates the chosen refinement process for the selected problem.
    -   `st.button("Run Benchmark Evaluation")`: Triggers the quantitative comparison on a subset of the dataset.

### Visualization Components (charts, graphs, tables)
-   **Raw Dataset Display:**
    -   `st.dataframe(humaneval_dataset['test'].to_pandas().head())`: Shows a sample of the HumanEval dataset.
-   **Code Display:**
    -   `st.code(code_string, language="python")`: Used to display initial LLM-generated code, property checks, and refined code attempts.
-   **Refinement History Table:**
    -   `st.dataframe(pd.DataFrame(refinement_history))`: Displays the iteration, code attempt (truncated), and detailed feedback.
-   **Quantitative Comparison Tables:**
    -   `st.dataframe(pbt_eval_df)` and `st.dataframe(tdd_eval_df)`: Show evaluation results for PBT and TDD on a benchmark subset.
    -   `st.dataframe(combined_eval_df)`: Displays combined results for easier comparison.
-   **Performance Comparison Plots (using `matplotlib.pyplot` and `seaborn`):**
    -   Line plots: `st.pyplot(fig_line)` for "Performance Trends Over Iterations" (e.g., `correctness_rate`, `repair_success_rate` vs. `Iteration`).
    -   Bar plots: `st.pyplot(fig_bar)` for "Average Performance Comparison" (average metrics for PBT vs. TDD).
    -   Bar plots for `visualize_test_coverage_and_feedback`: `st.pyplot(fig)` for "Test Pass Rate", "Property Violation Frequency", and "Semantic Feedback Efficiency".

### Interactive Elements and Feedback Mechanisms
-   **Loading/Processing Status:** `st.spinner("Loading dataset...")`, `st.info("Running refinement...")`, `st.success("Refinement successful!")`, `st.error("Refinement failed.")`.
-   **Detailed Feedback:** `st.expander("Detailed Feedback")` to reveal lists of `property_violations`, `runtime_errors`, and `timeouts`.
-   **Code History:** `st.expander("Code Attempts History")` to show previous versions of generated code.

## 3. Additional Requirements

### Annotation and Tooltip Specifications
-   **Mathematical Formulas:** All LaTeX equations will be rendered using `st.latex()` or `st.markdown` with appropriate delimiters. Tooltips/annotations for these will be provided in accompanying `st.markdown` text.
-   **Input Widgets:** `help` parameter will be used for `st.number_input`, `st.selectbox`, etc., to provide context (e.g., `st.number_input("Max Iterations", help="Maximum number of attempts the LLM will make to refine the code.")`).
-   **Visualizations:** Plot titles, axis labels, and legends will be clear. `st.pyplot` outputs will be accompanied by descriptive `st.markdown` text to interpret the graphs.

### Save the states of the fields properly so that changes are not lost
-   **Session State:** `st.session_state` will be extensively used to maintain the state of:
    -   `humaneval_dataset`: Once loaded.
    -   `selected_problem`: The currently chosen HumanEval problem.
    -   `max_iterations`: User-defined maximum iterations.
    -   `timeout_seconds`: User-defined test execution timeout.
    -   `current_code`: The LLM's latest code attempt.
    -   `refinement_history`: A list of dictionaries tracking code evolution and feedback.
    -   `pbt_eval_df`, `tdd_eval_df`, `combined_eval_df`: Results of benchmark evaluations.
    -   `selected_method`: The chosen testing method (PBT or TDD).
-   **Caching:**
    -   `@st.cache_data` will be used for `load_humaneval_dataset()` to prevent reloading on every rerun.
    -   `@st.cache_resource` might be considered if LLM models were actually loaded, but for this simulation, `cache_data` is sufficient for results.

import streamlit as st
st.set_page_config(page_title="QuLab", layout="wide")
st.sidebar.image("https://www.quantuniversity.com/assets/img/logo5.jpg")
st.sidebar.divider()
st.title("QuLab")
st.divider()
st.markdown("""
In this lab, we explore **Property-Based Testing (PBT)** as an advanced approach to refine code generated by Large Language Models (LLMs), contrasting it with Traditional Test-Driven Development (TDD).

### Why Property-Based Testing for LLM-Generated Code?
LLMs are powerful, but their code outputs can sometimes be subtly incorrect, handling edge cases poorly, or failing under specific input distributions. Traditional unit tests might miss these issues, as they often focus on specific examples.

PBT, on the other hand, defines *properties* that the code should always satisfy, regardless of the input. It then generates a diverse range of inputs to try and *falsify* these properties. This approach is particularly effective for LLM-generated code because:

*   **Comprehensive Coverage:** PBT explores a much wider input space than hand-written examples, increasing the likelihood of finding bugs.
*   **Robustness:** It helps ensure the code behaves correctly under unexpected or unusual conditions.
*   **Automated Feedback:** When a property is violated, PBT provides concrete examples (failing inputs), which can be fed back to the LLM for targeted refinement.

### The Iterative Refinement Process
We will simulate an iterative refinement loop using the **PGS (Propose, Generate, Synthesize)** framework, where two LLM-powered agents collaborate:

1.  **Generator Agent:** Responsible for synthesizing initial code and refining it based on feedback.
2.  **Tester Agent:** Responsible for generating diverse test inputs and validating code properties. In the PBT context, this agent generates inputs to try and *break* the defined properties.


Through this interactive application, you will:

*   **Understand PBT vs. TDD:** Grasp the fundamental differences and advantages of PBT.
*   **Visualize Refinement:** See how code evolves with PBT-driven feedback.
*   **Compare Effectiveness:** Quantitatively evaluate PBT against TDD using metrics like `correctness_rate` and `repair_success_rate`.
*   **Explore HumanEval:** Utilize this benchmark dataset for code generation tasks.

Let's dive in and explore how PBT can lead to more robust and reliable LLM-generated code!

**Note:** All formulae in markdown are enclosed in `$...$` or `$$...$$` for proper Streamlit rendering. For example, the `correctness_rate` can be defined as:

$$ \text{Correctness Rate} = \frac{\text{Number of Correct Solutions}}{\text{Total Number of Problems}} $$
""")
# Your code starts here
page = st.sidebar.selectbox(label="Navigation", options=["1. Application Overview", "2. Environment Setup", "3. Data/Inputs Overview", "4. Methodology Overview", "5. Sectioned Implementation", "6. Quantitative Comparison", "7. Visualization of Results", "8. Summary and Insights"])

if page == "1. Application Overview":
    from application_pages.page1_overview import run_page
    run_page()
elif page == "2. Environment Setup":
    from application_pages.page2_env_setup import run_page
    run_page()
elif page == "3. Data/Inputs Overview":
    from application_pages.page3_data_inputs import run_page
    run_page()
elif page == "4. Methodology Overview":
    from application_pages.page4_methodology import run_page
    run_page()
elif page == "5. Sectioned Implementation":
    from application_pages.page5_implementation import run_page
    run_page()
elif page == "6. Quantitative Comparison":
    from application_pages.page6_quantitative_comparison import run_page
    run_page()
elif page == "7. Visualization of Results":
    from application_pages.page7_visualization import run_page
    run_page()
elif page == "8. Summary and Insights":
    from application_pages.page8_summary import run_page
    run_page()
# Your code ends
